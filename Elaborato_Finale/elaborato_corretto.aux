\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {chapter}{Ringraziamenti}{ii}{chapter*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Indice}{iii}{chapter*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Introduzione}{1}{chapter*.3}\protected@file@percent }
\newlabel{cap:introduzione}{{}{1}{Introduzione}{chapter*.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Machine Learning}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{machinelearning}{{1}{2}{Machine Learning}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Come funziona il Machine Learning}{2}{section.1.1}\protected@file@percent }
\newlabel{ComefunzionailMachineLearning}{{1.1}{2}{Come funziona il Machine Learning}{section.1.1}{}}
\citation{SupervisedMachineLearning}
\citation{SupervisedMachineLearning}
\citation{Introductiontodatamining}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Apprendimento supervisionato}{3}{subsection.1.1.1}\protected@file@percent }
\newlabel{apprendimentoSupervisionato}{{1.1.1}{3}{Apprendimento supervisionato}{subsection.1.1.1}{}}
\citation{LectureNotesNg}
\citation{LectureNotesNg}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.1}Support Vector Machine}{4}{subsubsection.1.1.1.1}\protected@file@percent }
\newlabel{sec:SVC}{{1.1.1.1}{4}{Support Vector Machine}{subsubsection.1.1.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A sinistra grafico bidimensionale e destra tridimensionale\relax }}{4}{figure.caption.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Iperpiano e margine\relax }}{5}{figure.caption.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Infinite rette possono separare gli elementi\relax }}{6}{figure.caption.6}\protected@file@percent }
\citation{LectureNotesNg}
\citation{DataMiningandKnowledgeDiscoveryHandbook}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Non sempre \IeC {\`e} possibile dividere i dati linearmente\relax }}{8}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.2}Decision Tree Classifier}{8}{subsubsection.1.1.1.2}\protected@file@percent }
\citation{Introductiontodatamining}
\citation{Introductiontodatamining}
\citation{DataMiningandKnowledgeDiscoveryHandbook}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Visualizzazione di un albero di decisione\relax }}{9}{figure.caption.8}\protected@file@percent }
\citation{RandomForest}
\citation{Mitchell97}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Random Forest composto da pi\IeC {\`u} alberi di decisione\relax }}{10}{figure.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.3}Random Forest Classifier}{10}{subsubsection.1.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.4}Gaussian Naive Bayes}{10}{subsubsection.1.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.5}Linear Discriminat Analysis}{11}{subsubsection.1.1.1.5}\protected@file@percent }
\citation{multilayerPerceptron}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1.6}Reti Neurali Multi-Strato}{12}{subsubsection.1.1.1.6}\protected@file@percent }
\newlabel{MLP}{{1.1.1.6}{12}{Reti Neurali Multi-Strato}{subsubsection.1.1.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Visualizzazione di un percettrone\relax }}{12}{figure.caption.10}\protected@file@percent }
\citation{multilayerPerceptron}
\citation{unsupervisedlearning}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Visualizzazione di una rete multistrato; in rosso il livello di input, al centro il livello hidden, in verde a destra il livello di output\relax }}{13}{figure.caption.11}\protected@file@percent }
\citation{unsupervisedlearning}
\citation{semisupervised}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Apprendimento non supervisionato}{14}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Apprendimento per rinforzo}{14}{subsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Apprendimento semi supervisionato}{14}{subsection.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Dataset}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{dataset}{{2}{15}{Dataset}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Iris}{15}{section.2.1}\protected@file@percent }
\newlabel{iris}{{2.1}{15}{Iris}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Visualizzazione di un estratto del dataset Iris\relax }}{15}{figure.caption.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Incidenti Stradali}{16}{section.2.2}\protected@file@percent }
\newlabel{incidentiStradali}{{2.2}{16}{Incidenti Stradali}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Suddivisione caratteristiche dettagliate\relax }}{16}{figure.caption.13}\protected@file@percent }
\citation{shalevshwartz2014understanding}
\citation{pca}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Riduzione della Dimensionalit\IeC {\`a}}{17}{section.2.3}\protected@file@percent }
\newlabel{sec:riduzione}{{2.3}{17}{Riduzione della Dimensionalit√†}{section.2.3}{}}
\citation{pcaFigura}
\citation{pcaFigura}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}PCA}{18}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Dimensioni trovate tramite PCA \cite  {pcaFigura}\relax }}{18}{figure.caption.14}\protected@file@percent }
\citation{t-SNE}
\citation{t-SNE}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}t-SNE}{20}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Grafici della densit\IeC {\`a} delle distribuzioni gaussiana ( curva verde), e t-Student (curva blu). La forma della distribuzione t-Student \IeC {\`e} data dai gradi di libert\IeC {\`a} che si riferiscono al numero di osservazioni indipendenti all'interno dei dati\relax }}{20}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Esperimenti}{22}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Esperimenti}{{3}{22}{Esperimenti}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Preprocessing}{22}{section.3.1}\protected@file@percent }
\newlabel{preprocessing}{{3.1}{22}{Preprocessing}{section.3.1}{}}
\citation{scikit-learn}
\citation{scikit-learn}
\citation{scikit-learn}
\citation{scikit-learn}
\citation{Raschka2018ModelEM}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Scalare i dati}{23}{subsection.3.1.1}\protected@file@percent }
\citation{scikit-learn}
\citation{scikit-learn}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Model Selection}{24}{section.3.2}\protected@file@percent }
\newlabel{modelselection}{{3.2}{24}{Model Selection}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Scelta degli iperparametri}{24}{subsection.3.2.1}\protected@file@percent }
\newlabel{iperparametri}{{3.2.1}{24}{Scelta degli iperparametri}{subsection.3.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Errore di Generalizzazione}{25}{section.3.3}\protected@file@percent }
\newlabel{sec:errore}{{3.3}{25}{Errore di Generalizzazione}{section.3.3}{}}
\citation{TheElementsofStatisticalLearning}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Cross Validation}{26}{section.3.4}\protected@file@percent }
\newlabel{cv}{{3.4}{26}{Cross Validation}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Cross validation con 3 fold\relax }}{26}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Risultati Ottenuti}{27}{section.3.5}\protected@file@percent }
\newlabel{sec:risultati}{{3.5}{27}{Risultati Ottenuti}{section.3.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Risultati ottenuti StandardScaler, in azzurro vengono evidenziati gli score massimi ottenuti uno specifico modello usato come indice delle righe mentre in arancione lo score massimo assoluto\relax }}{27}{table.caption.18}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Risultati massimi, tra tutti i modelli utilizzati, ottenuti dai dataset usati come indice delle righe\relax }}{28}{table.caption.19}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Risultati ottenuti usando MinMaxScaler, in azzurro vengono evidenziati gli score massimi ottenuti uno specifico modello usato come indice delle righe mentre in arancione lo score massimo assoluto\relax }}{28}{table.caption.21}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Risultati massimi, tra tutti i modelli utilizzati, ottenuti dai dataset usati come indice delle righe\relax }}{29}{table.caption.22}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Risultati ottenuti usando RobustScaler, in azzurro vengono evidenziati gli score massimi ottenuti uno specifico modello usato come indice delle righe mentre in arancione lo score massimo assoluto\relax }}{29}{table.caption.24}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Risultati massimi, tra tutti i modelli utilizzati, ottenuti dai dataset usati come indice delle righe\relax }}{30}{table.caption.25}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Risultati ottenuti senza usare uno scaler di dati, in azzurro vengono evidenziati gli score massimi ottenuti uno specifico modello usato come indice delle righe mentre in arancione lo score massimo assoluto\relax }}{30}{table.caption.26}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Risultati massimi, tra tutti i modelli utilizzati, ottenuti dai dataset usati come indice delle righe\relax }}{31}{table.caption.27}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Visualizzazione generale dei risultati massimi ottenuti tra tutti i dataset con i vari scaler\relax }}{31}{table.caption.28}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Visualizzazione generale dei risultati massimi ottenuti tra tutti i modelli con i vari scaler e la loro media\relax }}{32}{table.caption.29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Analisi dei Risultati}{32}{section.3.6}\protected@file@percent }
\citation{scikit-learn}
\citation{jupyter}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Tecnologie Utilizzate}{33}{section.3.7}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{bibliografia}
\@writefile{toc}{\contentsline {chapter}{Conclusioni}{34}{chapter*.30}\protected@file@percent }
\bibcite{SupervisedMachineLearning}{1}
\bibcite{Introductiontodatamining}{2}
\bibcite{LectureNotesNg}{3}
\bibcite{DataMiningandKnowledgeDiscoveryHandbook}{4}
\bibcite{RandomForest}{5}
\bibcite{Mitchell97}{6}
\bibcite{multilayerPerceptron}{7}
\bibcite{unsupervisedlearning}{8}
\bibcite{semisupervised}{9}
\bibcite{shalevshwartz2014understanding}{10}
\bibcite{pca}{11}
\bibcite{pcaFigura}{12}
\bibcite{t-SNE}{13}
\bibcite{scikit-learn}{14}
\bibcite{Raschka2018ModelEM}{15}
\bibcite{TheElementsofStatisticalLearning}{16}
\bibcite{jupyter}{17}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{36}{chapter*.31}\protected@file@percent }
